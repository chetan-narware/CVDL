{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9q9UI_1gjO60"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Set correct permissions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "! kaggle datasets download tongpython/cat-and-dog"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PlopvDTjlfl",
        "outputId": "26a95e70-e11c-4a04-ab28-d1701d931cf7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/tongpython/cat-and-dog\n",
            "License(s): CC0-1.0\n",
            "Downloading cat-and-dog.zip to /content\n",
            " 94% 204M/218M [00:00<00:00, 270MB/s]\n",
            "100% 218M/218M [00:00<00:00, 258MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "dataset_path = \"/content\"  # Adjust if needed\n",
        "zip_file = os.path.join(dataset_path, r\"/content/cat-and-dog.zip\")\n",
        "\n",
        "# Extract main dataset\n",
        "with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(dataset_path)\n"
      ],
      "metadata": {
        "id": "pI_zwIR5jxMM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "\n",
        "#Step 1: Define the path to the dataset\n",
        "dataset_path = \"/content/training_set/training_set\"\n",
        "\n",
        "#Step 2: Image Data Generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "#Step 3: Load Training and Validation Data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "# Step 4: Define the CNN Model (Using MobileNetV2 for better performance)\n",
        "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(150, 150, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Model Architecture\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation=\"sigmoid\")  # Binary classification (0: Cat, 1: Dog)\n",
        "])\n",
        "\n",
        "# Step 5: Compile the Model\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Step 6: Train the Model\n",
        "history = model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
        "\n",
        "# Step 7: Evaluate Model\n",
        "loss, accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI9NDB6fko0I",
        "outputId": "550719b8-6938-4b26-ec36-7d63e75a52b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6404 images belonging to 2 classes.\n",
            "Found 1601 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-0275dd8779f8>:40: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(150, 150, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 291ms/step - accuracy: 0.8710 - loss: 0.2757 - val_accuracy: 0.9332 - val_loss: 0.1504\n",
            "Epoch 2/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 240ms/step - accuracy: 0.9246 - loss: 0.1804 - val_accuracy: 0.9432 - val_loss: 0.1454\n",
            "Epoch 3/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 242ms/step - accuracy: 0.9329 - loss: 0.1647 - val_accuracy: 0.9388 - val_loss: 0.1348\n",
            "Epoch 4/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 242ms/step - accuracy: 0.9387 - loss: 0.1510 - val_accuracy: 0.9438 - val_loss: 0.1466\n",
            "Epoch 5/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 239ms/step - accuracy: 0.9425 - loss: 0.1401 - val_accuracy: 0.9375 - val_loss: 0.1373\n",
            "Epoch 6/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 239ms/step - accuracy: 0.9462 - loss: 0.1293 - val_accuracy: 0.9413 - val_loss: 0.1396\n",
            "Epoch 7/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 239ms/step - accuracy: 0.9438 - loss: 0.1296 - val_accuracy: 0.9319 - val_loss: 0.1372\n",
            "Epoch 8/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 238ms/step - accuracy: 0.9450 - loss: 0.1268 - val_accuracy: 0.9475 - val_loss: 0.1276\n",
            "Epoch 9/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 244ms/step - accuracy: 0.9465 - loss: 0.1250 - val_accuracy: 0.9394 - val_loss: 0.1377\n",
            "Epoch 10/10\n",
            "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 238ms/step - accuracy: 0.9508 - loss: 0.1250 - val_accuracy: 0.9438 - val_loss: 0.1353\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 193ms/step - accuracy: 0.9333 - loss: 0.1530\n",
            "Validation Accuracy: 93.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AjpUJU9DodWd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}